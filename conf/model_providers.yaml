# Supported model providers for Briven
# ---------------------------------------
#
# Each provider type ("chat", "embedding") contains a mapping of provider IDs
# to their configurations.
#
# The provider ID (e.g., "anthropic") is used:
#  - in the settings UI dropdowns.
#  - to construct the environment variable for the API key (e.g., ANTHROPIC_API_KEY).
#
# Each provider configuration requires:
#   name:             Human-readable name for the UI.
#   litellm_provider: The corresponding provider name in LiteLLM.
#
# Optional fields:
#   kwargs:           A dictionary of extra parameters to pass to LiteLLM.
#                     This is useful for `api_base`, `extra_headers`, etc.

chat:
  briven_venice:
    name: Briven API
    litellm_provider: openai
    kwargs:
      api_base: https://llm.briven.ai/v1
      venice_parameters:
        include_venice_system_prompt: false
  anthropic:
    name: Anthropic
    litellm_provider: anthropic
  cometapi:
    name: CometAPI
    litellm_provider: cometapi
  deepseek:
    name: DeepSeek
    litellm_provider: deepseek
  github_copilot:
    name: GitHub Copilot
    litellm_provider: github_copilot
    kwargs:
      extra_headers:
        "Editor-Version": "vscode/1.85.1"
        "Copilot-Integration-Id": "vscode-chat"
        "Copilot-Vision-Request": "true"
  google:
    name: Google
    litellm_provider: gemini
  groq:
    name: Groq
    litellm_provider: groq
  huggingface:
    name: HuggingFace
    litellm_provider: huggingface
  lm_studio:
    name: LM Studio
    litellm_provider: lm_studio
  mistral:
    name: Mistral AI
    litellm_provider: mistral
  moonshot:
    name: Moonshot AI
    litellm_provider: moonshot
  ollama:
    name: Ollama
    litellm_provider: ollama
  perplexity:
    name: Perplexity AI
    litellm_provider: perplexity
  cohere:
    name: Cohere
    litellm_provider: cohere
  openai:
    name: OpenAI
    litellm_provider: openai
  azure:
    name: OpenAI Azure
    litellm_provider: azure
  bedrock:
    name: AWS Bedrock
    litellm_provider: bedrock
  openrouter:
    name: OpenRouter
    litellm_provider: openrouter
    kwargs:
      extra_headers:
        "HTTP-Referer": "https://briven.ai/"
        "X-Title": "Briven"
  sambanova:
    name: Sambanova
    litellm_provider: sambanova
  venice:
    name: Venice.ai
    litellm_provider: openai
    kwargs:
      api_base: https://api.venice.ai/api/v1
      venice_parameters:
        include_venice_system_prompt: false
  # xAI / Grok models â€” set API_KEY_XAI in .env
  # Chat models: grok-4-0709, grok-3, grok-3-mini, grok-2-vision-1212
  # Fast/reasoning: grok-4-1-fast-reasoning, grok-4-1-fast-non-reasoning
  # Coding: grok-code-fast-1
  # Vision-capable: grok-4-0709, grok-2-vision-1212, grok-4-1-fast-*
  # No embedding models available from xAI (use another provider for embeddings)
  xai:
    name: xAI
    litellm_provider: xai
  zai:
    name: Z.AI
    litellm_provider: openai
    kwargs:
      api_base: https://api.z.ai/api/paas/v4
  zai_coding:
    name: Z.AI Coding
    litellm_provider: openai
    kwargs:
      api_base: https://api.z.ai/api/coding/paas/v4
  other:
    name: Other OpenAI compatible
    litellm_provider: openai

embedding:
  huggingface:
    name: HuggingFace
    litellm_provider: huggingface
  google:
    name: Google
    litellm_provider: gemini
  lm_studio:
    name: LM Studio
    litellm_provider: lm_studio
  mistral:
    name: Mistral AI
    litellm_provider: mistral
  ollama:
    name: Ollama
    litellm_provider: ollama
  openai:
    name: OpenAI
    litellm_provider: openai
  azure:
    name: OpenAI Azure
    litellm_provider: azure
  bedrock:
    name: AWS Bedrock
    litellm_provider: bedrock
  # TODO: OpenRouter not yet supported by LiteLLM, replace with native litellm_provider openrouter and remove api_base when ready
  openrouter:
    name: OpenRouter
    litellm_provider: openai
    kwargs:
      api_base: https://openrouter.ai/api/v1
      extra_headers:
        "HTTP-Referer": "https://briven.ai/"
        "X-Title": "Briven"
  other:
    name: Other OpenAI compatible
    litellm_provider: openai
